# 7. Common Problems When Building a RAG System

## Here’s an overview of the key challenges and potential pitfalls:

### **1 Poor Chunking Strategy**

- **Problem:** Overly large chunks can exceed LLM token limits or dilute relevance, while very small chunks lose contextual coherence.
- **Impact:** Retrieval becomes noisy or non-specific, leading to irrelevant or fragmented LLM responses.
- **Solution:**
    - Use recursive text splitting strategies (e.g., `RecursiveCharacterTextSplitter`) to create chunks with **manageable size and overlap.**
    - Preserve **hierarchical context**, such as headings or sections, in each chunk.

---

### **2 Low-Quality Embeddings**

- **Problem:** Using inappropriate or generic embedding models leads to poor relevance scores during semantic similarity search.
- **Impact:** Results retrieved from the vector database may not align with the user’s query, confusing the LLM.
- **Solution:**
    - Select embeddings fine-tuned for specific tasks (e.g., sentence embeddings for text, table embeddings for tabular data).
    - Regularly evaluate embedding performance with **human feedback loops**.

---

### **3 Metadata Gaps**

- **Problem:** Missing or inconsistent metadata (e.g., file author, timestamp, project ID) hinders filtering and personalized retrieval.
- **Impact:** Unfiltered retrieval mixes irrelevant or outdated data with recent, relevant chunks.
- **Solution:**
    - Enforce **rigorous metadata enrichment** during ingestion pipelines. Attach key details like:
        - File type (e.g., PDF, Word)
        - Source (e.g., SharePoint, Jira)
        - Timestamps
    - Build validation checks to ensure metadata consistency.

---

### **4 Context Window Overflow**

- **Problem:** Retrieving too many chunks for a query can exceed an LLM’s token limit (e.g., GPT-4’s ~32k tokens), causing truncation and loss of critical context.
- **Impact:** Responses may lack key details, leading to incomplete or incorrect outputs.
- **Solution:**
    - Prioritize retrieved chunks based on their **similarity scores** and user intent.
    - Apply a **token budget** to limit the total context size passed to the LLM.

---

### **5 Hallucination**

- **Problem:** The LLM fabricates information when the retrieved data is ambiguous, incomplete, or irrelevant.
- **Impact:** Users receive false or misleading answers, reducing trust in the system.
- **Solution:**
    - Ensure retrieval fetches **sufficient, relevant context.**
    - Consider adding **source citations** to verify outputs, e.g., "Based on Project X Policy, Section 3."

---

### **6 Outdated Knowledge**

- **Problem:** Vector databases may serve stale or irrelevant data if the ingestion pipeline isn’t refreshing content regularly.
- **Impact:** Users retrieve and rely on outdated information, especially in fast-changing environments like legal updates or product releases.
- **Solution:**
    - Design ingestion pipelines with **periodic refresh intervals.**
    - Use **event-driven updates** for systems like SharePoint or Jira to sync changes in real time.

---

### **7 Access Control Leakage**

- **Problem:** Insufficient enforcement of user permissions can expose sensitive or restricted information during retrieval.
- **Impact:** Major security risks, including potential legal and compliance violations.
- **Solution:**
    - Incorporate strict **permissions filtering** tied to metadata.
    - For SaaS apps like SharePoint, use **user access tokens** to enforce platform-level access.

---

### **8 Latency Issues**

- **Problem:** Slow retrieval, embedding generation, or multiple calls to the LLM bottleneck the user experience.
- **Impact:** High response times lead to user frustration, especially in real-time applications like chatbots.
- **Solution:**
    - Precompute and store embeddings for static documents.
    - Optimize database indexes for fast similarity search.
    - Limit LLM calls by handling **simple queries** directly in the retrieval layer.

---

### **9 Data Format Mismatch**

- **Problem:** Inconsistent input or output formats—e.g., for SQL, JSON APIs, or tabular data.
- **Impact:** Downstream workflows break or require post-processing.
- **Solution:**
    - Standardize parsing outputs (e.g., JSON structures for databases or SQL formats for NLP-to-SQL workflows).
    - Use structured prompts like:
        
        ```
        Return SQL for the user query below:
        Query: {query}
        Table Schema: {schema_description}
        
        ```
        
---

### **10 Embedding Drift**

- **Problem:** Updating embedding models without re-indexing the vector database creates mismatches between stored embeddings and queried embeddings.
- **Impact:** Poor retrieval accuracy and degraded system performance.
- **Solution:**
    - When updating embedding models, schedule a **re-indexing pipeline** to refresh all stored embeddings.
    - Test retrieval performance with the new embeddings before deployment.

---

## **Application-Specific Problems**

Let’s zoom in on some common **use-case-specific pitfalls**:

---

### ** File-Based Knowledge Assistant**

- **OCR Errors:** Poorly scanned documents or handwritten content produce low-quality text for chunking.
- **Loss of Formatting:** Tables, charts, and images may not translate well into retrievable text during parsing.
- **Missing Metadata:** Critical filtering factors like department ownership, file versions, or update timestamps are frequently skipped.

**How to Solve:**

- Use **Azure Document Intelligence** or **Tesseract OCR** for reliable text extraction.
- Build pipelines for preserving non-text elements like tables (e.g., using **Camelot** for table parsing).
- Enforce metadata tagging workflows during ingestion.

---

### **2 NLP-to-SQL Chatbot**

- **Schema Retrieval Issues:** If the schema isn’t retrieved correctly, the LLM may fail to generate accurate queries.
- **LLM Hallucinations:** Generated queries often reference non-existent tables or columns.
- **Execution Speed:** Poorly optimized queries cause backend database slowdowns.

**How to Solve:**

- Store and retrieve database schema metadata as structured embeddings.
- Validate generated SQL queries before execution using a **SQL parser.**
- Apply query optimization techniques.

---

### **3 SharePoint Chatbot**

- **Complex Permissions:** Nested groups or user-level access rules can complicate retrieval pipelines.
- **API Rate Limits:** Syncing massive data stores from SharePoint may exceed API quotas.
- **Stale Content:** Search indices don’t account for real-time document updates.

**How to Solve:**

- Integrate SharePoint Graph API to enforce user-level permissions at retrieval.
- Use event-driven triggers for syncing and incremental updates instead of full refreshes.

---

### **4 Jira or Asana RAG Bots**

- **Task Metadata Challenges:** Some fields, like priority or status, may be missing or inconsistent between projects.
- **APIs Pagination:** Large project backlogs require multiple API calls to retrieve all tasks.
- **Mapping Queries:** Translating user intent into Jira/Asana project IDs or task filters is often tricky.

**How to Solve:**

- Normalize metadata across projects before storing in the vector database.
- Batch API calls using pagination libraries (e.g., `requests` for Python) to fetch all data.
- Build LLM prompt templates that effectively map natural language queries into API-compliant filters.

---

## **Conclusion**

Building a RAG system is both rewarding and challenging—each component, from **chunking strategies** to **real-time ingestion pipelines**, must align to offer seamless, accurate, and secure outcomes. By identifying and addressing these pitfalls early, you’ll ensure your system delivers **trustworthy, efficient, and scalable results** across every use case.