# 6. How to build a RAG Backend

## **What is Retrieval-Augmented Generation (RAG)?**

RAG combines **retrieval-based algorithms** with **generative AI models**. In simple terms:

1. **Retrieval Phase:** Relevant context is fetched from external knowledge sources (e.g., files, APIs, vector databases).
2. **Augmentation Phase:** The retrieved context is combined with user queries and passed to a **Large Language Model (LLM)** (e.g., GPT-4) to generate accurate, grounded responses.

This architecture bridges static LLM knowledge with real-time, domain-specific data, making it highly adaptable for **chatbots**, **NLP-to-SQL assistants**, or any application requiring up-to-date information.

---

## **Core RAG Backend Architecture**

---

## Step 1: Saving Webpages as Text

The first step is to fetch content directly from the web. We’ll use a function called **`save_webpages_as_text`**, which:

- Takes in a list of URLs.
- Extracts the readable text content.
- Saves each webpage into a `.txt` file inside a `datasets/` folder locally.

This ensures that we always have the webpage content stored offline for processing.

```python
import requests
from bs4 import BeautifulSoup

def save_webpages_as_text(urls):
    for url, filename in urls.items():
        try:
            print(f"Fetching webpage content from URL: {url}")
            response = requests.get(url)
            response.raise_for_status()

            print(f"Parsing HTML content from: {url}")
            soup = BeautifulSoup(response.text, "html.parser")
            text = soup.get_text(separator="\n")

            print(f"Saving extracted text to file: dataset/{filename}")
            with open(f"dataset/{filename}", "w", encoding="utf-8") as file:
                file.write(text)

            print(f"Successfully saved webpage content to: {filename}")
        except Exception as e:
            print(f"Error processing URL {url}: {str(e)}")
```

---

## Step 2: Downloading and Parsing PDFs

Many important documents (like L\&T’s reports, brochures, or manuals) are available as PDFs. We’ll create a function called **`download_and_parse_pdfs`** that:

- Downloads the PDFs from given URLs.
- Uses the `pypdf` library to extract text.
- Saves each PDF’s content into `.txt` files in the same `datasets/` folder.

This step helps us bring structured knowledge from PDFs into a machine-readable format.

```python
from PyPDF2 import PdfReader

def download_and_parse_pdfs(pdf_urls):
    for url, filename in pdf_urls.items():
        pdf_filepath = f"dataset/{filename}"  # Save inside dataset folder
        txt_filename = filename.replace(".pdf", ".txt")  # Convert to text file name
        txt_filepath = f"dataset/{txt_filename}"

        try:
            # Step 1: Download the PDF
            print(f"Downloading PDF from URL: {url}")
            response = requests.get(url, stream=True)
            response.raise_for_status()

            with open(pdf_filepath, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)

            print(f"Successfully downloaded PDF file: {pdf_filepath}")

            # Step 2: Extract text from the PDF
            print(f"Extracting text content from PDF: {pdf_filepath}")
            with open(pdf_filepath, "rb") as pdf_file:
                reader = PdfReader(pdf_file)
                extracted_text = "\n".join([page.extract_text() or "" for page in reader.pages])

            # Step 3: Save extracted text to a .txt file
            print(f"Saving extracted PDF text to: {txt_filepath}")
            with open(txt_filepath, "w", encoding="utf-8") as txt_file:
                txt_file.write(extracted_text)

            print(f"Successfully extracted and saved text from PDF: {filename}")

        except Exception as e:
            print(f"Error processing PDF {url}: {str(e)}")
```

---

## Step 3: Orchestrating with `process_text_files`

Instead of calling the above functions separately, we wrap them with a single function: **`process_text_files`**.

This function takes in a set of URLs (both webpage links and PDF links), and behind the scenes it calls:

1. `save_webpages_as_text`
2. `download_and_parse_pdfs`

At the end of this step, all text content—whether from webpages or PDFs—will be neatly stored in `.txt` files inside the `datasets/` folder.

```python
import re
import json

def process_text_files(urls):
    """
    Processes all URLs, sending PDFs to download_and_parse_pdfs and webpages to save_webpages_as_text.
    """
    pdf_urls = {url: filename for url, filename in urls.items() if url.endswith('.pdf')}
    webpage_urls = {url: filename for url, filename in urls.items() if not url.endswith('.pdf')}

    # Process PDFs
    if pdf_urls:
        print(f"Processing {len(pdf_urls)} PDF documents")
        download_and_parse_pdfs(pdf_urls)

    # Process Webpages
    if webpage_urls:
        print(f"Processing {len(webpage_urls)} webpages")
        save_webpages_as_text(webpage_urls)
```

---

## Step 4: Chunking the Data

Large text documents are not directly suitable for RAG. LLMs work best when they are given smaller, semantically meaningful **chunks of text**.

For this, we use the **`chunk_data`** function. Here’s what it does:

- Takes a mapping of `url → filename`.
- Reads the text from each file.
- Uses a **Recursive Character Text Splitter** to break the text into overlapping chunks.
- Each chunk is stored along with useful metadata:

```json
{
  "chunk_data": "The actual text chunk...",
  "metadata": {
    "filename": "example.txt",
    "url": "<https://example.com>"
  }
}

```

Finally, all chunks are saved into a single JSON file. This JSON can later be ingested into a vector database (like Chroma, Pinecone, or FAISS) to enable semantic search and retrieval.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def preprocess_text(text, filename, url):
    text = text.lower()
    text = re.sub(r'\n{3,}', '\n\n', text)  # Limit newlines to max 2 consecutive

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=32
    )
    chunks = text_splitter.split_text(text)
    
    chunks_data = []

    # Append data to the global list
    for chunk in chunks:
        chunks_data.append({
            "chunk_data": chunk,
            "metadata": {
                "filename": filename,
                "url": url
            }
        })

    print(f"Created {len(chunks)} chunks from file: {filename}")
    return chunks_data

def chunk_data(urls):
    all_chunks_data = []
    print("Starting to chunk text files for knowledge base")
    for url, filename in urls.items():
        if filename.endswith('.txt'):
            try:
                print(f"Processing text file for chunking: {filename}")
                with open(f"dataset/{filename}", "r", encoding="utf-8") as file:
                    text = file.read()
                chunks_data = preprocess_text(text, filename, url)
                all_chunks_data += chunks_data
                print(f"Added {len(chunks_data)} chunks from {filename} to knowledge base")
            except Exception as e:
                print(f"Error chunking file {filename}: {str(e)}")

    print(f"Saving {len(all_chunks_data)} total chunks to JSON file")
    with open("dataset/all_chunks_data.json", "w", encoding="utf-8") as json_file:
        json.dump(all_chunks_data, json_file, ensure_ascii=False, indent=4)

    print("Successfully saved all chunks to 'all_chunks_data.json'")
    
```

## Step 5: Creating Embeddings from JSON

Now that we have all our chunks stored in a JSON file, the next step is to convert them into **embeddings**—numerical vector representations that capture the semantic meaning of text.

We’ll use a function called **`create_embeddings_from_json`**, which:

- Reads the chunks from the JSON file.
- Uses a **Sentence Transformer model** to generate embeddings.
- Stores the embeddings inside a **ChromaDB collection** for fast semantic search.

This step is the bridge between raw text data and intelligent retrieval.

```python
import json
from sentence_transformers import SentenceTransformer
import chromadb

def create_embeddings_from_json(json_filename, model, collection):
    print(f"Creating embeddings from JSON file: {json_filename}")
    
    # Load the chunk data from the JSON file
    with open(json_filename, 'r', encoding='utf-8') as file:
        chunk_data = json.load(file)

    # Prepare sentences (chunks) and metadata for embedding
    print(f"Preparing {len(chunk_data)} chunks for embedding generation")
    sentences = [chunk['chunk_data'] for chunk in chunk_data]
    metadatas = [chunk['metadata'] for chunk in chunk_data]

    # Generate embeddings using the SentenceTransformer model
    print("Generating embeddings using SentenceTransformer model")
    embeddings = model.encode(sentences)

    # Insert the embeddings and metadata into the Chroma collection
    print(f"Adding {len(sentences)} embeddings to ChromaDB collection")
    for idx, embedding in enumerate(embeddings):
        collection.add(
            ids=[str(idx)],
            embeddings=[embedding],
            metadatas=[metadatas[idx]],
            documents=[sentences[idx]]
        )

    print(f"Successfully created embeddings for {len(sentences)} chunks and added to Chroma collection")

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

client = chromadb.PersistentClient(path="LNTRAG_CHROMADB")
collection = client.get_or_create_collection(name="LNT_COLLECTION")

create_embeddings_from_json('dataset/all_chunks_data.json', model, collection)

num_documents = len(collection.get()['documents'])
print(f"Dataset creation complete. Knowledge base contains {num_documents} documents")
```

---

## Step 6: Querying the Chroma Database

Once our chunks are embedded and stored in ChromaDB, we can query them.

The function **`query_chroma_db`** does the following:

- Takes a user query.
- Converts it into an embedding using the same Sentence Transformer model.
- Performs a **semantic similarity search** across the collection.
- Returns the top\_k most relevant results.

This makes it possible to retrieve passages that are conceptually close to the query, not just exact keyword matches.

```python
def query_chroma_db(embedding_model, collection, query, top_k):
    print(f"Fetching top {top_k} relevant queries for: '{query}'")
        
    # Convert query text into an embedding vector
    print("Generating embeddings for query")
    query_embedding = embedding_model.encode([query], show_progress_bar=False)

    # Perform similarity search in ChromaDB collection
    print("Performing similarity search in ChromaDB collection")
    search_results = collection.query(
        query_embeddings=query_embedding,
        n_results=top_k
    )

    # Process and structure the search results
    relevant_queries = [
        {
            "document": doc,
            "metadata": search_results['metadatas'][index],
            "score": search_results['distances'][index]
        }
        for index, doc in enumerate(search_results['documents'])
    ]
    
    print(f"Found {len(relevant_queries)} relevant documents based on embedding similarity")

    return relevant_queries
```

---

## Step 7: Reranking with a Cross-Encoder

While semantic search is powerful, sometimes the top results need better fine-tuning. That’s where a **Cross Encoder** comes in.

We’ll wrap the retrieval function inside **`query_and_rerank_chroma_db`**, which works as follows:

1. Calls `query_chroma_db` to fetch the top\_k candidate chunks.
2. Uses the **Cross Encoder** to score each result in the context of the query.
3. Reranks the results based on the `cross_encoder_score`.

This two-step process—**semantic search + reranking**—is a common and effective approach in modern RAG pipelines.

```python
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb

def query_and_rerank_chroma_db(cross_encoder, embedding_model, collection, query, top_k):
    print(f"Getting top {top_k} most relevant content for query: '{query}'")
        
    # Fetch top-k results based on embedding similarity
    print("First pass: fetching candidate results using embedding similarity")
    top_results = query_chroma_db(embedding_model, collection, query, top_k)

    # Rank results using a cross-encoder model
    print("Second pass: re-ranking candidates using cross-encoder model")
    ranked_results = [
        {
            "document": top_results[0]['document'][i],
            "metadata": top_results[0]['metadata'][i],
            "similarity_score": top_results[0]['score'][i],
            "cross_encoder_score": cross_encoder.predict([(query, top_results[0]['document'][i])])[0]
        }
        for i in range(len(top_results[0]['document']))
    ]

    # Sort by cross-encoder score and return top-k
    sorted_results = sorted(ranked_results, key=lambda x: x["cross_encoder_score"], reverse=True)[:top_k]
    print(f"Retrieved final {len(sorted_results)} most relevant content items after re-ranking")
    
    return sorted_results
    
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

client = chromadb.PersistentClient(path="LNTRAG_CHROMADB")
collection = client.get_or_create_collection(name="LNT_COLLECTION")

cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

query_and_rerank_chroma_db(cross_encoder, model, collection, "Who is chairman of L&T", top_k=3)

```

## Step 8: Using Retrieved Results with Azure OpenAI

The last step is to **connect retrieval with generation**. Until now, we prepared data, created embeddings, and built a strong retrieval pipeline with reranking. Now we’ll feed the retrieved content into an **LLM prompt** to generate an informed answer.

We’ll use **Azure OpenAI** for this. The workflow looks like this:

1. Take the user query.
2. Use `query_and_rerank_chroma_db` to fetch the top relevant chunks.
3. Append these chunks to the query as **context**.
4. Send the combined prompt to the Azure OpenAI model.
5. Return the final grounded response.

Here’s the code structure:

```python
import os
from dotenv import load_dotenv
from openai import AzureOpenAI

# Load environment variables
load_dotenv()

endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")

# Initialize client
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=api_key,
)

prompt = "Who is the chairman of L&T"

# Step 1: Retrieve and rerank relevant chunks
relevant_content = query_and_rerank_chroma_db(cross_encoder, model, collection, prompt, top_k=3)

# Step 2: Attach content to the user query
prompt += f" attached content: {relevant_content}"

# Step 3: Send query + context to the model
response = client.chat.completions.create(
    messages=[
        {"role": "system", "content": "You are a helpful assistant. Use the attached content if needed."},
        {"role": "user", "content": prompt},
    ],
    model=deployment,
)

# Step 4: Print the answer
print(response.choices[0].message.content)

```

---

## End-to-End Flow

At this point, we’ve built a **complete Retrieval-Augmented Generation (RAG) pipeline** for L\&T data:

1. **Data ingestion** → Webpages + PDFs → text files.
2. **Preprocessing** → Chunk data into JSON with metadata.
3. **Embeddings** → Use Sentence Transformers to create vectors.
4. **Vector DB** → Store embeddings in ChromaDB.
5. **Retrieval** → Query embeddings and fetch relevant results.
6. **Reranking** → Improve ordering with a Cross Encoder.
7. **LLM Integration** → Send retrieved context + query to Azure OpenAI.

The result is an **intelligent assistant** that can answer L\&T-specific queries, backed by the company’s own documents.

---

### **3️⃣ Maintenance and Scalability**

1. **Automatic Data Refresh**
    - Ensure periodic ingestion of new files or API data.
2. **Retraining or Fine-Tuning**
    - Regularly fine-tune embeddings or apply feedback loops for improved retrieval and LLM performance.
3. **Scaling Infrastructure**
    - Use **containerization** (Docker, Kubernetes) to scale individual stages (e.g., parallelized embedding generation).

---

## **Advantages of RAG Backend**

1. **Dynamic Updates**: Combines live external knowledge with LLM capabilities.
2. **Cost Savings**: Reduces unnecessary calls to LLM by pre-filtering data.
3. **Versatility**: Adapts to files, SaaS platforms, or even hybrid datasets.
4. **Context-Rich Outputs**: Responses are always grounded in real, retrievable data.

---

## **Example Applications**

1. **File-Based Knowledge Assistant**
    - Process manuals, compliance documents, or employee handbooks.
2. **NLP-to-SQL Chatbot**
    - Generate SQL queries from user language by retrieving schema descriptions and aligning intent.
3. **SharePoint Chatbot**
    - Answer HR or policy-related queries by querying SharePoint-hosted data with permissions.
4. **Jira or Asana RAG Bots**
    - Retrieve task/project details dynamically based on space/project IDs.

---

## **Conclusion**

Building a robust RAG backend empowers you to leverage generative AI for **real-world, context-rich applications**. Following a modular approach—**chunking, embedding, metadata tagging, vector storage, and retrieval**—allows you to create highly adaptable systems across diverse data sources.

💡 Ready to build your own RAG backend? Let me know if you'd like help setting up pipelines, embedding flows, or vector databases! 🚀