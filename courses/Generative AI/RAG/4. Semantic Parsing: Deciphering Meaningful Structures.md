# 4. Semantic Parsing: Deciphering Meaningful Structures

**Semantic Parsing** refers to the process of converting unstructured text or natural-language queries into a structured, machine-readable format. This is foundational for applications like:

- **Converting natural language into SQL queries.**
- **Extracting hierarchical relationships from text files.**
- **Understanding documents (e.g., policies, manuals) and retrieving context-aware answers.**

Semantic Parsing plays a critical role in **RAG** systems because it helps maintain the **meaning** and **context** of the text being analyzed, ensuring responses are grounded in reality.

---

## **Chunking in Semantic Parsing**

When dealing with large documents, it's impossible (and inefficient) to process the entire text at once. Instead, we split the document into **chunks** or smaller segments. While splitting can work smoothly for small, non-structured chunks, challenges arise when dealing with **hierarchical data** (headings, subheadings, etc.).

Let’s take **Recursive Character Text Splitting** as an example of document chunking.

### **Code Example: Recursive Character Text Splitting**

Consider a file with a heading/subheading structure:

```
## Heading 1
This is the introduction to Heading 1. This text gives a summary.

### Subheading 1.1
Further explanation about Subheading 1.1. More content follows.

### Subheading 1.2
Another subheading 1.2 explanation.

## Heading 2
Details about Heading 2 are here.

```

When we use **recursive character text splitting**, the content is **split purely based on character limits**, as shown in this example:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Simulated text content
content = """
## Heading 1
This is the introduction to Heading 1. This text gives a summary.

### Subheading 1.1
Further explanation about Subheading 1.1. More content follows.

### Subheading 1.2
Another subheading 1.2 explanation.

## Heading 2
Details about Heading 2 are here.
"""

# Initialize text splitter
splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
chunks = splitter.split_text(content)

# Display the chunks
for idx, chunk in enumerate(chunks):
    print(f"Chunk {idx+1}:\\n{chunk}\\n")

```

### **Output of Recursive Character Splitting**

```
Chunk 1:
## Heading 1
This is the introduction to Heading 1. This text gives a summary.
### Subheading 1.1
Further explanation about Su

Chunk 2:
bheading 1.1. More content follows.
### Subheading 1.2
Another subheading 1.2 explan

Chunk 3:
ation.
## Heading 2
Details about Heading 2 are here.

```

### **Challenges**

- **Context Lost:** The splitter divides the document based purely on size, without regard for structure. The **headings ("Heading 1", "Subheading 1.1")** are arbitrarily split or lost entirely.
- **Loss of Hierarchy:** It’s unclear which chunks belong to which headings and subheadings. This is critical when handling hierarchical data repositories (e.g., FAQ docs or database schemas).

---

## **How To Address The Chunking Challenge**

To ensure proper **context preservation**, we must **integrate the heading and subheading metadata into each chunk** as it’s created. This way, for every chunk, you know where the snippet belongs in the document structure.

### **Chunking with Heading/Subheading Capture**

Here’s a strategy to extract chunks while retaining the full hierarchy.

```python
import re

# Function to capture headings and associate them with chunks
def structured_chunking(content, chunk_size=100, overlap=20):
    # Regex for headings
    heading_pattern = r"^(#+) (.*)"  # Matches "## Heading" or "### Subheading"

    headings = []
    chunks = []
    current_context = ""  # Keep track of the active heading

    lines = content.split("\\n")
    text = ""
    for line in lines:
        match = re.match(heading_pattern, line)
        if match:
            if text.strip():
                # Save the previous chunk along with the current context
                chunks.append({"context": current_context.strip(), "chunk": text.strip()})
                text = ""
            # Update context with the new heading
            heading_level, heading_text = match.groups()
            current_context += f"{'  ' * (len(heading_level) - 1)}{heading_text} > "
        else:
            text += line + " "

    # Add the final chunk
    if text.strip():
        chunks.append({"context": current_context.strip(), "chunk": text.strip()})

    return chunks

# Test it on a file
file_content = """
## Heading 1
This is the introduction to Heading 1. This text gives a summary.

### Subheading 1.1
Further explanation about Subheading 1.1. More content follows.

### Subheading 1.2
Another subheading 1.2 explanation.

## Heading 2
Details about Heading 2 are here.
"""

chunked_data = structured_chunking(file_content)

# Display the result
for chunk in chunked_data:
    print(f"Context: {chunk['context']}\\nChunk: {chunk['chunk']}\\n")

```

### **Output with Structured Chunking**

```
Context: Heading 1 >
Chunk: This is the introduction to Heading 1. This text gives a summary.

Context: Heading 1 > Subheading 1.1 >
Chunk: Further explanation about Subheading 1.1. More content follows.

Context: Heading 1 > Subheading 1.2 >
Chunk: Another subheading 1.2 explanation.

Context: Heading 2 >
Chunk: Details about Heading 2 are here.

```

---

## **Adapting This for RAG Systems**

By capturing the **heading/subheading context**, this approach ensures context-rich responses in various **RAG applications** like:

### **1. NLP to SQL Chatbot**

- For a query like, *"How do I query sales by region?"*, the bot retrieves not just a SQL snippet but the heading context (e.g., **"Section: SQL for Sales Data"**).

### **2. SharePoint Chatbot**

- When searching company policies, chunks can be linked to their hierarchical location on SharePoint (e.g., **"HR > Leave Policies > Annual Leave"**).

### **3. Files Chatbot**

- For FAQ documents or legal contracts, linking each chunk to its respective section/title ensures clarity and accuracy in answers.

### **4. Confluence Chatbot**

- Each Confluence page is structured into hierarchical headings. Capturing and correlating chunks with headings helps generate accurate responses with inline references.

### **5. Asana/Jira Chatbot**

- While querying large task descriptions or sprint documents, chunks mapped to **project or task hierarchy** can be retrieved, keeping task context intact.

---

## **Advantages and Disadvantages**

### **Advantages of Heading-Aware Chunking**

1. **Context Clarity:** Preserving heading/subheading hierarchy ensures users get meaningful responses that align with document structure.
2. **Improved Search Results:** Hierarchically chunked data improves semantic search and information retrieval accuracy.
3. **Scalability:** Works effectively across domains (SQL, SharePoint, Confluence, etc.).

### **Disadvantages**

1. **Performance Overhead:** For large documents, parsing and associating context for every chunk can increase execution time.
2. **Complexity in Dynamic Data:** Requires consistent heading/subheading formats (regex parsing could fail for inconsistent text).

---

## **Wrapping It Up**

Semantic parsing with heading-aware chunking is a game-changer for **context-preserving RAG architectures**. From NLP-to-SQL chatbots to SharePoint document retrieval tools, this approach ensures each chunk retains its contextual breadcrumb trail—making systems smarter and responses more grounded.