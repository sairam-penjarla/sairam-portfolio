# Connecting Tools to Your Agent: Making Agents Act

We've built an agent that can converse and remember. We've created tools that can fetch real-world data. Now it's time to bring them together—to give our agent the power to act.

This is the moment when your agent transforms from a chatbot into something more powerful: an autonomous system that can perceive what it needs, select the right tool, execute actions, and synthesize results into useful responses.

## The Complete Tool Setup

Let's start with our weather tool, fully implemented:

```python
from agents import function_tool
import requests

api_key = "PASTE YOUR API KEY HERE"

@function_tool
def get_weather(city_name: str) -> dict:
    base_url = "https://api.openweathermap.org/data/2.5/weather"
    params = {
        "q": city_name,
        "appid": api_key,
        "units": "metric"  # use "imperial" for Fahrenheit
    }

    response = requests.get(base_url, params=params)
    response.raise_for_status()  # raise error for non-200 status
    data = response.json()

    return {
        "city": data["name"],
        "temperature": data["main"]["temp"],
        "description": data["weather"][0]["description"],
        "humidity": data["main"]["humidity"],
        "wind_speed": data["wind"]["speed"]
    }

```

We've seen this before—a weather API function decorated with `@function_tool` to make it agent-compatible. Now let's connect it to our agent.

## Creating a Tool-Enabled Agent

Here's where the magic happens:

```python
custom_agent = Agent(
    name="My Custom Agent",
    instructions="You are an AI agent that can use tools to answer user questions. Use the provided tool to get weather information when asked.",
    model=OpenAIChatCompletionsModel(model=model_name, openai_client=agent_client),
    tools=[get_weather]
)

```

### What Changed?

Compare this to our original agent definition from Blog 2:

**Before (No Tools):**

```python
custom_agent = Agent(
    name="My Custom Agent",
    instructions="You are a conversational AI assistant. Answer the user's questions based on your knowledge",
    model=OpenAIChatCompletionsModel(model=model_name, openai_client=agent_client),
)

```

**After (With Tools):**

```python
custom_agent = Agent(
    name="My Custom Agent",
    instructions="You are an AI agent that can use tools to answer user questions. Use the provided tool to get weather information when asked.",
    model=OpenAIChatCompletionsModel(model=model_name, openai_client=agent_client),
    tools=[get_weather]  # ← NEW!
)

```

Two critical changes:

### 1. The `tools` Parameter

```python
tools=[get_weather]

```

This single line does something profound: it gives the agent **access** to the weather tool. The tools parameter accepts a list of decorated functions. The agent framework:

- Registers each tool with the agent
- Makes tool schemas available to the language model
- Sets up the execution pipeline for tool calls
- Enables the agent to see tool descriptions and signatures

Think of this as giving the agent a toolbox. The agent can now "see" that it has a `get_weather` function available, along with its description and parameter requirements.

### 2. Updated Instructions

```python
instructions="You are an AI agent that can use tools to answer user questions. Use the provided tool to get weather information when asked."

```

The instructions now explicitly mention tool usage. This is important! While the agent can technically discover tools through their schemas, explicitly mentioning them in instructions:

- Encourages the agent to consider using tools
- Provides context on when tools should be used
- Sets expectations for the agent's behavior
- Reduces hesitation or confusion about tool usage

Think of instructions as the agent's training—you're teaching it not just *what* tools it has, but *how* to think about using them.

## Watching the Agent in Action

Let's see our tool-enabled agent respond to a query:

```python
result = await Runner.run(
    custom_agent,
    input="hey hi. What's the weather in Tokyo?"
)
display(Markdown(result.final_output))

```

### What Happens Under the Hood?

When you run this, here's the complete execution flow:

**Step 1: Query Analysis**

The agent receives: "hey hi. What's the weather in Tokyo?"

The language model processes this and identifies:

- A greeting ("hey hi")
- A weather information request
- A specific location ("Tokyo")

**Step 2: Tool Discovery**

The agent examines its available tools and finds `get_weather`:

```
Function: get_weather
Parameters: city_name (string)
Description: Get current weather information for a specific city

```

**Step 3: Decision Making**

The agent reasons:

- "The user wants weather information"
- "I have a get_weather tool"
- "The tool accepts a city_name parameter"
- "The user specified Tokyo"
- "Decision: Use get_weather with city_name='Tokyo'"

**Step 4: Tool Execution**

The agent framework:

1. Calls `get_weather("Tokyo")`
2. The function makes an HTTP request to OpenWeatherMap API
3. The API returns weather data
4. The function formats and returns the result
5. The framework captures the output

**Step 5: Result Integration**

The agent receives the tool output:

```python
{
    "city": "Tokyo",
    "temperature": 25,
    "description": "Light rain",
    "humidity": 70,
    "wind_speed": 8
}

```

**Step 6: Response Generation**

The agent synthesizes a natural language response incorporating:

- The greeting acknowledgment
- The weather data from the tool
- Natural, conversational formatting

**Possible Output:**

```
Hi there! The current weather in Tokyo is 25°C with light rain. The humidity
is at 70% and there's a moderate wind blowing at 8 m/s. You might want to
bring an umbrella if you're heading out!

```

## The Power of Autonomous Tool Selection

Notice what we *didn't* do:

- ❌ We didn't explicitly tell the agent to call `get_weather`
- ❌ We didn't write conditional logic checking for weather keywords
- ❌ We didn't manually extract "Tokyo" and pass it as a parameter

The agent did all of this autonomously. It:

- ✅ Understood the user's intent
- ✅ Recognized which tool could help
- ✅ Extracted the city name from natural language
- ✅ Called the tool with the correct parameter
- ✅ Interpreted the results
- ✅ Generated a natural response

This is fundamentally different from traditional programming, where you'd write explicit if-statements and parsing logic.

## Testing Different Scenarios

Let's explore how the agent handles various situations:

### Scenario 1: Direct Weather Query

**Input:** "What's the weather in London?"

**Agent Process:**

1. Identifies weather query
2. Calls `get_weather("London")`
3. Returns formatted weather information

**Expected Behavior:** Direct tool use, straightforward response.

### Scenario 2: Conversational Context

**Input:** "I'm planning a trip to Paris. What's the weather like?"

**Agent Process:**

1. Understands implicit weather query in travel context
2. Extracts "Paris" as the location
3. Calls `get_weather("Paris")`
4. Responds with weather information relevant to travel planning

**Expected Behavior:** The agent infers intent and extracts location from context.

### Scenario 3: No Tool Needed

**Input:** "What's the capital of France?"

**Agent Process:**

1. Identifies factual question
2. Recognizes no tool is required (knows from training data)
3. Responds directly: "The capital of France is Paris."

**Expected Behavior:** Agent doesn't use tools unnecessarily.

### Scenario 4: Ambiguous Location

**Input:** "What's the weather like?"

**Agent Process:**

1. Identifies weather query
2. Recognizes missing location information
3. Responds: "I'd be happy to check the weather for you! Which city would you like to know about?"

**Expected Behavior:** Agent asks clarifying questions instead of guessing.

### Scenario 5: Multiple Cities

**Input:** "Compare the weather in Tokyo and London"

**Agent Process:**

1. Identifies request for multiple cities
2. Calls `get_weather("Tokyo")`
3. Calls `get_weather("London")`
4. Synthesizes comparison response

**Expected Behavior:** Agent orchestrates multiple tool calls autonomously.

## The Tool Execution Lifecycle

Let's trace the complete journey from query to response:

```
┌─────────────────────────────────────────────────────────────┐
│ 1. USER INPUT                                               │
│    "What's the weather in Tokyo?"                           │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 2. AGENT REASONING                                          │
│    - Parse query intent                                     │
│    - Identify information need (weather data)               │
│    - Check available tools                                  │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 3. TOOL SELECTION                                           │
│    - Match need to tool: get_weather                        │
│    - Extract parameters: city_name = "Tokyo"                │
│    - Validate tool can satisfy request                      │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 4. TOOL INVOCATION                                          │
│    - Framework calls: get_weather("Tokyo")                  │
│    - Function executes (API call)                           │
│    - Returns structured data                                │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 5. RESULT PROCESSING                                        │
│    - Receive tool output                                    │
│    - Validate and parse data                                │
│    - Store in working memory                                │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 6. RESPONSE SYNTHESIS                                       │
│    - Combine tool results with context                      │
│    - Generate natural language                              │
│    - Format for user display                                │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 7. OUTPUT                                                   │
│    "The current weather in Tokyo is 25°C with light rain"  │
└─────────────────────────────────────────────────────────────┘

```

## Understanding Tool Call Results

When you inspect the result object, you'll see evidence of the tool call:

```python
result = await Runner.run(custom_agent, input="What's the weather in Tokyo?")

# The result object contains:
# - final_output: The natural language response
# - messages: Conversation history including tool calls
# - tool_calls: Detailed information about which tools were used

```

The conversation history will show something like:

```python
[
    {
        "role": "user",
        "content": "What's the weather in Tokyo?"
    },
    {
        "role": "assistant",
        "content": null,
        "tool_calls": [
            {
                "id": "call_abc123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"city_name": "Tokyo"}'
                }
            }
        ]
    },
    {
        "role": "tool",
        "tool_call_id": "call_abc123",
        "content": '{"city": "Tokyo", "temperature": 25, ...}'
    },
    {
        "role": "assistant",
        "content": "The current weather in Tokyo is 25°C with light rain..."
    }
]

```

Notice the `tool` role—this captures the tool's output in the conversation flow, allowing the agent to reference it when generating the final response.

## Adding Multiple Tools

The real power emerges when agents have access to multiple tools:

```python
@function_tool
def get_weather(city_name: str) -> dict:
    # Weather tool implementation
    pass

@function_tool
def calculate(expression: str) -> float:
    # Calculator tool implementation
    pass

@function_tool
def search_web(query: str) -> dict:
    # Web search tool implementation
    pass

custom_agent = Agent(
    name="Multi-Tool Agent",
    instructions="You are an AI agent with multiple tools. Use get_weather for weather queries, calculate for math problems, and search_web for current information.",
    model=OpenAIChatCompletionsModel(model=model_name, openai_client=agent_client),
    tools=[get_weather, calculate, search_web]
)

```

Now the agent can handle diverse queries:

- "What's the weather in Paris?" → Uses `get_weather`
- "What's 15% of 250?" → Uses `calculate`
- "Who won the election?" → Uses `search_web`
- "If the weather in Tokyo is above 20°C, calculate 20% tip on a $50 meal" → Uses both `get_weather` and `calculate` in sequence

The agent autonomously selects the right tool(s) for each situation.

## Best Practices for Tool Integration

### 1. Clear Tool Instructions

Be explicit in your agent instructions:

✅ Good:

```python
instructions="""
You are an AI agent with access to these tools:
- get_weather: Use for current weather information
- calculate: Use for mathematical computations
- search_web: Use for recent news or information not in your training data

Always use tools when they can provide more accurate or current information.
"""

```

❌ Vague:

```python
instructions="You have some tools. Use them sometimes."

```

### 2. Tool Naming Conventions

Use descriptive, action-oriented names:

✅ Good: `get_weather`, `calculate_percentage`, `search_database`
❌ Bad: `tool1`, `helper`, `func`

### 3. Error Handling in Tools

Ensure tools return structured error information:

```python
@function_tool
def get_weather(city_name: str) -> dict:
    try:
        # API call logic
        return weather_data
    except requests.exceptions.HTTPError as e:
        return {
            "error": True,
            "message": f"Failed to fetch weather: {e}",
            "city": city_name
        }

```

This allows the agent to respond gracefully: "I'm sorry, I couldn't retrieve the weather for that city. Please check the spelling or try another location."

### 4. Parameter Validation

Validate within the tool:

```python
@function_tool
def get_weather(city_name: str) -> dict:
    if not city_name or not isinstance(city_name, str):
        return {"error": True, "message": "Invalid city name"}

    if len(city_name) > 100:
        return {"error": True, "message": "City name too long"}

    # Proceed with API call

```

### 5. Consistent Return Structures

All your tools should follow similar patterns:

```python
# Success response
{
    "success": True,
    "data": {...}
}

# Error response
{
    "success": False,
    "error": "Error message"
}

```

This makes it easier for the agent to handle responses uniformly.

## What We've Accomplished

Today, we crossed a critical threshold:

- ✅ Connected tools to our agent
- ✅ Understood the tool invocation lifecycle
- ✅ Saw autonomous tool selection in action
- ✅ Learned how agents reason about tool use
- ✅ Explored multiple tool scenarios
- ✅ Established best practices for tool integration

Our agent is no longer just conversational—it's **capable**. It can reach out into the world, fetch information, perform actions, and synthesize results.

## What's Next

We've built a tool-enabled agent, but we've glossed over something important: how do you create **robust, testable tools** without always depending on external APIs? How do you simulate complex API behaviors—rate limits, errors, edge cases?

In the next blog, we'll dive deep into **simulating API calls effectively**, building mock tools that behave like real ones, and creating test harnesses for your agent systems.

After that, we'll explore **advanced tool creation techniques**—tricks to make your tools more reliable, efficient, and agent-friendly.

Then we'll expand our horizons even further: **multiple agents**, **handoffs**, and **orchestration**. That's when things get really exciting.

---

*Next: Simulating API Calling in Tools—building robust, testable tool implementations that behave like real-world APIs.*