# 5. Observing Intermediate Steps in AI Agents

In the realm of AI agents, we've talked about their autonomy, their ability to use tools, and how context empowers them. But for all their intelligence, agents can sometimes feel like a "black box." You give them a prompt, and a response eventually appears. What happens in between? How do they decide which tool to call? When do they switch agents? And how can you, as a developer or even a curious user, gain insight into this complex internal dance?

This is where **observing intermediate steps** becomes critical. It's about pulling back the curtain and seeing the agent's thought process, tool interactions, and state changes in real-time. This visibility is not just a debugging luxury; it's essential for building reliable, transparent, and user-friendly AI applications.

---

### Why Observe Intermediate Steps?

Observing what an agent is doing behind the scenes offers several significant benefits:

1. **Transparency:** Users feel more in control and trusting when they understand *how* an AI is arriving at an answer or performing a task. "The AI is thinking..." is less helpful than "The AI is now searching the web..."
2. **Debugging Complex Workflows:** Agents can involve multi-step reasoning, chained tool calls, and even handoffs between specialized agents. When something goes wrong, tracing these intermediate steps is the only way to pinpoint the exact point of failure.
3. **Enhanced User Experience:** For long-running tasks, providing real-time updates (e.g., "Generating image...", "Consulting knowledge base...") keeps the user engaged and informed, reducing perceived latency.
4. **Performance Optimization:** By observing how and when tools are called, you can identify inefficient patterns or unnecessary steps, leading to cost and latency savings.
5. **Agent Instruction Refinement:** Seeing an agent's internal monologue and tool choices helps you refine its instructions (`agent_instructions.md`) to guide its behavior more effectively.

---

### The Power of Streaming: `Runner.run_streamed`

To gain this real-time visibility, modern AI agent frameworks often provide a streaming execution mode. Instead of waiting for the entire process to complete and receiving a single final output, you receive a continuous stream of "events" as the agent progresses.

In the provided code, `Runner.run_streamed` is the key:

```python
# Assuming 'main_agent' and 'user_info' are defined elsewhere
result = Runner.run_streamed(main_agent, input=messages, context=user_info)
```

- **Asynchronous Execution:** The `run_streamed` method is typically asynchronous (`async`), meaning your application can continue performing other tasks while the agent processes the request.
- **Real-time Event Stream:** It returns a `RunResultStreaming` object (or similar), which exposes an asynchronous iterator that yields `StreamEvent` objects as they occur. Each `StreamEvent` represents a specific moment or action within the agent's execution.

---

### Decoding the Event Stream: Types of Intermediate Events

The `stream_events()` method provides a powerful way to tap into the agent's internal workings. Let's look at the different types of events you might receive and what they signify, as shown in your code snippet:

```python
# ... (initial setup) ...

async for event in result.stream_events():
    # 1. Raw Response Events (LLM output tokens)
    if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
        print(f"[RAW RESPONSE EVENT]---> {event.data.delta}")
        yield event.data.delta # Yielding to a frontend or client
        partial_message += event.data.delta

    # 2. Agent Updated Events (Agent Handoffs)
    elif event.type == "agent_updated_stream_event":
        print(f"[AGENT UPDATES] {event.new_agent.name}")
        yield "[AGENT UPDATES]" + event.new_agent.name
        time.sleep(0.25) # Small delay for better visibility

    # 3. Run Item Events (Tool Calls, Messages, Reasoning)
    elif event.type == "run_item_stream_event":
        item = event.item
        print(f"[RUN ITEM EVENT] {item}")

        # Sub-type: Tool Call Output
        if item.type == "tool_call_output_item":
            yield "[TOOL OUTPUT]" + json.dumps(str(item.output))
            time.sleep(0.25)

        # Sub-type: Message Output Delta (Final agent message chunks)
        elif item.type == "message_output_delta_item":
            delta = item.delta
            if delta and delta.content:
                yield delta.content
                partial_message += delta.content

# ... (final message reconstruction) ...
```

1. **`raw_response_event` (and `ResponseTextDeltaEvent`)**:
    - **What it is:** These are the most granular events, representing token-by-token outputs directly from the underlying Large Language Model (LLM). This is similar to how a chatbot types out its response character by character.
    - **Why observe:** Perfect for building real-time "typing" effects in a user interface, giving immediate feedback that the AI is processing.
2. **`agent_updated_stream_event`**:
    - **What it is:** This event is emitted when the active agent in a multi-agent system changes, typically due to a "handoff" from one specialized agent to another.
    - **Why observe:** Crucial for understanding the flow of control in complex systems. If a Triage Agent hands off to an Image Specialist Agent, this event signals that transition, allowing a UI to update its context or show which "expert" is now handling the request.
3. **`run_item_stream_event`**:
    - **What it is:** These are higher-level events that signal significant milestones in the agent's execution, wrapping a `RunItem` object. They provide structured information about actions taken.
    - **Sub-types within `run_item_stream_event`**:
        - **`tool_call_output_item`**: This is emitted *after* a tool has been successfully executed and its output is available. It contains the result of the tool's operation (e.g., the URL of the generated image from our `generate_image` tool).
            - **Why observe:** Essential for showing users the results of specific actions (e.g., "Image generated: [URL]"). Also vital for debugging tool integration.
        - **`message_output_delta_item`**: Similar to `raw_response_event` but often represents chunks of the final, user-facing message generated by the agent. It can be used to reconstruct the complete message.
            - **Why observe:** Helps in building the complete response that the user will eventually see.

---

### Reconstructing the Conversation and Final Output

After processing the stream, the agent framework often provides mechanisms to reconstruct the full interaction history and extract the final definitive output.

```python
# ... (after the async for loop) ...

# Retrieve the full trace (conversation history + intermediate steps)
trace = result.to_input_list()
# The code then processes 'trace' to extract 'new_message_trace'
# and determines the 'final_output' from the partial message accumulated
# or from the last assistant message in the trace.

final_output = partial_message # Or from trace reconstruction for robustness
print("[FINAL MESSAGE]" + final_output)
print("[NEW MESSAGES TRACE]" + json.dumps(new_message_trace))
```

- **`result.to_input_list()`**: This method (from the `RunResultStreaming` object) provides a complete chronological list of all inputs, outputs, tool calls, and agent messages that occurred during the run. This "trace" is invaluable for detailed post-mortem analysis and debugging.
- **Final Message Reconstruction:** The provided code snippet demonstrates how to reconstruct the `final_output` (the agent's ultimate response to the user) and `new_message_trace` (the relevant parts of the conversation that occurred during this specific run) from the stream events and the full trace. This ensures that even with streaming, you can capture the complete, coherent output.

---

### Why These Insights Matter for Your Agent Applications

Implementing event streaming and observing intermediate steps transforms your AI agent application from a black box into a transparent, engaging, and debuggable system.

- **For Developers:** It provides the necessary visibility to understand agent behavior, diagnose issues, and continuously improve the agent's performance and accuracy.
- **For Users:** It enhances the user experience by providing real-time feedback, showing the agent's progress, and building trust in its capabilities.

By embracing these observability patterns, you move beyond simply *using* AI agents to truly *mastering* their intricate and powerful workflows.