# 3. Building an AI Agent: Bringing Your Tools to Life

In our journey through the world of AI agents, we've explored what they are and how to create custom tools to extend their capabilities. Now, it's time to put it all together: building the AI agent itself and connecting those powerful tools so it can start taking action autonomously.

An AI agent isn't just a collection of tools; it's a sophisticated orchestrator that understands user intent, leverages its instructions, and intelligently decides which tools to employ. Let's dive into how this is done using Python, focusing on our previously discussed `generate_image` tool.

---

### The Core of Your Agent: The `Agent` Class

At the heart of an AI agent framework (like the one hinted at in our examples, often from an SDK like `openai-agents-python`) is typically an `Agent` class. This class is where you define the fundamental characteristics and abilities of your AI.

Here's the Python code snippet for defining our `image_specialist_agent`:

```python
import os
from openai import AsyncAzureOpenAI
from agents import Agent, OpenAIChatCompletionsModel, function_tool
from dataclasses import dataclass

# Placeholder for UserInfo dataclass (as provided)
@dataclass
class UserInfo:
    email: str
    auth_access_token: str
    entra_id_user_id: str
    user_id: str
    session_id: str

# Initialize the OpenAI client for the agent's underlying model
agent_client = AsyncAzureOpenAI(
    api_key=os.getenv("GPT_4_O_AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("GPT_4_O_AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("GPT_4_O_AZURE_OPENAI_API_ENDPOINT"),
    azure_deployment=os.getenv("GPT_4_O_AZURE_OPENAI_API_MODEL_NAME"),
)

# Load agent instructions from a markdown file
with open("agent_instructions.md", "r", encoding="utf-8") as file:
    agent_instructions = file.read()

# Get the model name from environment variables
model_name = os.getenv("GPT_4_O_AZURE_OPENAI_API_MODEL_NAME")

# --- Assume generate_image tool function is defined elsewhere and imported ---
# For illustration purposes, imagine 'generate_image' is imported from 'my_tools_module'
# from my_tools_module import generate_image

# Define the Agent
image_specialist_agent = Agent[UserInfo](
    name="Agent with art",
    instructions=agent_instructions,
    model=OpenAIChatCompletionsModel(model=model_name, openai_client=agent_client),
    tools=[generate_image], # This is how we connect our tool!
)

result = await Runner.run(
      image_specialist_agent,
      input=user_query,
      context=my_user_info # <-- Passing the UserInfo instance here
  )
  print(f"Agent response: {result.final_output}")
```

Let's break down the key parameters when defining an `Agent`:

- **`name`**: This is a unique identifier for your agent (e.g., "Agent with art"). It helps in logging, debugging, and, in multi-agent systems, allows other agents to refer to it.
- **`instructions`**: This is arguably the most critical component. It's the "system prompt" or the core directive that defines your agent's personality, purpose, and rules of engagement. We'll delve into this in detail next.
- **`model`**: This specifies the underlying Large Language Model (LLM) that powers your agent's reasoning capabilities. Here, `OpenAIChatCompletionsModel` is used, linked to an `AsyncAzureOpenAI` client and a specific deployment model (like GPT-4o). This LLM is the "brain" that interprets requests, understands context, and decides on actions.
- **`tools`**: This is where you list all the custom tools (like our `generate_image` function decorated with `@function_tool`) that this agent has access to. By providing this list, you're essentially giving the agent its "skill set."

---

### Guiding Your Agent: The `agent_instructions.md`

The `instructions` parameter is where you imbue your AI agent with its intelligence and specific behavioral guidelines. It's a detailed markdown file (`agent_instructions.md` in our example) that acts as the agent's manual, outlining its role, responsibilities, and how it should interact with users and tools.

Here's a look at the content of our `agent_instructions.md`:

```markdown
# **Agent Instructions – Image Specialist**

### **Role**

You are an AI assistant that can both **answer questions** and **generate images**.

---

### **Tool Access**

You have access to the `generate_image` tool, which takes:

* **prompt** *(str, required)* – A detailed, well-structured description of the image to create.
* **style** *(str, optional)* – Defaults to `"realistic"`.
* **size** *(str, optional)* – Defaults to `"1024x1024"`. Must be one of:

  * `"1024x1024"`
  * `"1792x1024"`
  * `"1024x1792"`

---

### **Behavior Rules**

1. **Understand the user request first**

   * If the message is purely a **question** (factual, conceptual, conversational), answer directly without generating an image.
   * If the request is **visual** (create, imagine, visualize), decide if `generate_image` is needed.

2. **When generating images**

   * Reformulate the request into a **clear, vivid, specific prompt**.
   * Fill in missing details with **reasonable assumptions** consistent with the user’s intent (lighting, perspective, colors, environment, etc.).
   * Match the style to the request, or default to `"realistic"`.
   * Ensure size is one of the valid values, defaulting to `"1024x1024"` if not specified.
   * Make the prompt **visually descriptive** – include **sensory and contextual details** (time of day, mood, background, focus subject).

3. **Answer + Image Hybrid**

   * If the user asks a **question + image**, first answer briefly, then call `generate_image`.

---

### **Output Expectations**

When calling the tool, use the exact structure:

```python
generate_image(
    prompt="<final descriptive prompt>",
    style="<style>",
    size="<size>"
)
```

* Do **not** include extra explanations alongside the tool call (system handles tool responses).

---

### **Prompt Refinement Guidelines**

* **Bad:** `"Draw a cat"`
* **Good:** `"A fluffy orange cat lounging on a sunlit windowsill, warm afternoon light streaming through, realistic style, soft focus background."`

**Always remove ambiguity** – specify:

* Subjects
* Setting
* Mood
* Style
* Perspective
* Relevant details
```

Key aspects of these instructions:

- **Role Definition:** Clearly states the agent's purpose ("AI assistant that can both answer questions and generate images").
- **Tool Manifest:** Explicitly lists the `generate_image` tool and its parameters. This is how the LLM inside the agent "learns" about the tool's capabilities.
- **Behavior Rules:** Provides a clear decision-making framework:
    - When to answer directly vs. when to use a tool.
    - Guidelines for prompt reformulation for image generation (emphasizing detail and clarity).
    - Handling hybrid requests (answer a question *then* generate an image).
- **Output Expectations:** Guides the agent on how to format its output when calling a tool, ensuring clean and correct arguments are passed.
- **Prompt Refinement Guidelines:** Offers concrete examples of good vs. bad prompts, training the agent to generate high-quality inputs for its tools.

These instructions are crucial because they dictate how the agent will interpret user requests and interact with the tools you provide. A well-crafted instruction set leads to a more intelligent and reliable agent.

---

### The Context Provider: `RunContextWrapper` and `UserInfo`

Our `generate_image` tool function from the previous blog had a unique first parameter: `wrapper: RunContextWrapper[UserInfo]`. This component is vital for providing dynamic, session-specific, or user-specific information to your tools and agent.

Here's the `UserInfo` dataclass defined for our agent:

```python
from dataclasses import dataclass

@dataclass
class UserInfo:
    email: str
    auth_access_token: str
    entra_id_user_id: str
    user_id: str
    session_id: str
```

- **`UserInfo` (`@dataclass`):** This is a simple data structure (made concise with the `@dataclass` decorator) that holds information about the current user and their session. This includes IDs, tokens, or any other relevant details.
- **`RunContextWrapper`:** The agent framework wraps your `UserInfo` (or any custom context object you define) within a `RunContextWrapper`. This wrapper is then automatically passed to your tool functions (and potentially other parts of the agent's lifecycle).

Why is this important?

- **Dynamic Data:** Tools often need information that changes per user or per interaction (e.g., a `user_id` for personalized storage, an `auth_token` for secure API calls).
- **Decoupling:** It decouples your tool logic from global state or direct access to user session details, making your tools more reusable and testable.
- **Traceability:** As seen in our `generate_image` tool, the `user_id` and `session_id` from `wrapper.context` were used to create unique and organized filenames in Azure Blob Storage, enhancing traceability and debugging.

When the agent executes a "run" based on a user's prompt, you'll typically pass an instance of `UserInfo` to the `Runner` (a component that executes the agent). The framework then ensures this context is available to any tools the agent decides to call.

---

### Conclusion: Your Agent, Ready to Act

You've now seen the full picture of building a capable AI agent. From defining its core with the `Agent` class and its underlying LLM, to meticulously crafting its instructions, and finally, connecting it to powerful, context-aware tools. This modular approach allows for flexible, scalable, and highly intelligent AI systems.

The `image_specialist_agent` is now "aware" of its purpose, "knows" how to behave, and has the "skill" (the `generate_image` tool) to create visual content. In our next blog, we'll dive into the **execution phase**: how a user interacts with this agent, how the agent's internal reasoning leads to tool calls, and what the complete workflow looks like when the agent brings its instructions and tools to life to fulfill a request.