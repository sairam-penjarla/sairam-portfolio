# Memory in AI Agents: Building Context and Continuity

In our previous blog, we built a functional AI agent that could respond to queries. But there was a limitation—each interaction was isolated. The agent had no memory of previous conversations, no understanding of context beyond a single exchange.

Today, we're fixing that. We're going to implement memory in our agent, transforming it from a simple responder into a conversational partner that remembers, learns, and maintains context across multiple interactions.

## The Problem: Stateless Agents

Let's recall what we had before:

```python
result = await Runner.run(custom_agent, input="hi")

```

This works great for single queries, but what happens when you want a conversation? If you asked about earthquakes and then said "tell me more about that," the agent would have no idea what "that" refers to. Each call to `Runner.run()` was independent—no shared context, no memory.

This is the fundamental difference between a chatbot and an agent. Chatbots can answer questions. Agents can have conversations.

## Implementing Memory: The Foundation

Here's how we add memory to our agent:

```python
messages = []

async def ask_agent(query: str):
    global messages
    input_msg = [{"role": "user", "content": query}]
    result = await Runner.run(custom_agent, input=messages + input_msg)
    messages = result.to_input_list()
    display(Markdown(result.final_output))

```

This simple wrapper function transforms our agent into a stateful system. Let's break down what's happening.

### The Memory Store

```python
messages = []

```

This list is our agent's memory—a simple but powerful structure that holds the entire conversation history. It's a growing record of everything that's been said: user queries, agent responses, tool calls, and results.

In production systems, you might store this in a database, Redis cache, or specialized vector store, but the concept remains the same: a persistent record of interactions.

### Building Context

```python
input_msg = [{"role": "user", "content": query}]
result = await Runner.run(custom_agent, input=messages + input_msg)

```

Here's where the magic happens. Instead of sending just the current query, we're doing something clever:

1. **Create the new message**: `input_msg` contains the user's current query
2. **Combine with history**: `messages + input_msg` creates a complete context—all previous interactions plus the new query
3. **Pass to agent**: The agent receives the full conversation history, allowing it to understand references, maintain context, and build on previous exchanges

This is the core pattern of conversational AI: always provide the full context, not just the latest input.

### Updating Memory

```python
messages = result.to_input_list()

```

After the agent responds, we update our memory with the complete conversation history. The `to_input_list()` method (which we explored in the previous blog) returns the structured conversation format, now including both the new user message and the agent's response.

This ensures that the next interaction will have access to everything that's been discussed so far.

## Memory in Action: A Multi-Turn Conversation

Let's see how memory transforms agent interactions. We'll have a three-turn conversation that demonstrates context awareness.

### Turn 1: The Greeting

```python
await ask_agent("hi")

```

**Agent Response:**

```
Hello! How can I help you today?

```

**Memory State:**

```json
[
  {
    "role": "user",
    "content": "hi"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Hello! How can I help you today?",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  }
]

```

Nothing surprising yet—a simple greeting exchange. But notice the structured format being stored in memory. Both the user's message and the agent's response are preserved with full metadata.

### Turn 2: Asking About Earthquakes

```python
await ask_agent("what is an earthquake")

```

**Agent Response:**

```
An earthquake is the shaking of the Earth's surface caused by the sudden release of energy
in the Earth's crust. This energy release usually happens along faults, which are cracks in
the Earth's crust where the ground has moved.

Earthquakes can vary in size from those that are so weak that you barely feel them to strong
ones that can cause severe damage and destruction. The point inside the Earth where the
earthquake starts is called the focus or hypocenter, while the point directly above it on the
surface is called the epicenter.

```

**Memory State:**

```json
[
  {
    "role": "user",
    "content": "hi"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Hello! How can I help you today?",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "role": "user",
    "content": "what is an earthquake"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "An earthquake is the shaking of the Earth's surface caused by the sudden release of energy in the Earth's crust. This energy release usually happens along faults, which are cracks in the Earth's crust where the ground has moved. \n\nEarthquakes can vary in size from those that are so weak that you barely feel them to strong ones that can cause severe damage and destruction. The point inside the Earth where the earthquake starts is called the focus or hypocenter, while the point directly above it on the surface is called the epicenter.\n\nCommon causes of earthquakes include:\n- Movement of tectonic plates\n- Volcanic activity\n- Human activities (such as mining or filling large reservoirs)\n\nSeismographs are instruments used to measure and record the strength and duration of earthquakes.",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  }
]

```

Now our memory contains four messages: two from the user, two from the agent. The conversation is building.

### Turn 3: The Power of Context

This is where memory really shines:

```python
await ask_agent("can you write a poem on it")

```

Notice the pronoun "it"—there's no explicit subject in this query. How does the agent know what "it" refers to? **Memory.**

**Agent Response:**

```
Absolutely! Here's a poem about earthquakes:

**Whispers Beneath the Earth**

Beneath our feet, the silence stirs,
A secret kept in stone and dirt,
Deep below, the pressures build—
Nature's breath, unseen, unthrilled.

A fleeting instant splits the night,
The ground begins its hurried flight,
Walls may sway and windows shake,
As earth reveals what lies awake.

Echoes rise from hidden veins,
A language spoken in the strains,
Cracks may form and fears may grow,
Yet from the tremor, life will flow.

For after chaos, calm returns,
The world relearns, the earth re-earns,
Her quiet pulse, her steady song,
Reminding us we all belong.

If you'd like a shorter version or a different style, let me know!

```

The agent correctly understood that "it" refers to earthquakes from the previous exchange. This is context awareness in action.

**Final Memory State:**

```json
[
  {
    "role": "user",
    "content": "hi"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Hello! How can I help you today?",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "role": "user",
    "content": "what is an earthquake"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "An earthquake is the shaking of the Earth's surface caused by the sudden release of energy in the Earth's crust...",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "role": "user",
    "content": "can you write a poem on it"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Absolutely! Here's a poem about earthquakes:\n\n**Whispers Beneath the Earth**\n\nBeneath our feet, the silence stirs...",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  }
]

```

Our memory now contains the complete conversation—six messages capturing the full interaction flow from greeting to creative output.

## How Memory Enables Context Understanding

Let's trace exactly how the agent resolved "it" in our poem request:

1. **User's query arrives**: "can you write a poem on it"
2. **Memory is loaded**: The agent receives all previous messages:
    - The greeting exchange
    - The earthquake explanation
    - The current request
3. **Context analysis**: The language model analyzes the conversation flow:
    - Most recent substantive topic: earthquakes
    - User is now asking about "it" with no new topic introduced
    - Logical inference: "it" = earthquakes
4. **Response generation**: With full context, the agent confidently writes a poem about earthquakes

Without memory, the agent would have responded: "I'd be happy to write a poem, but what would you like it to be about?"

## Memory Architecture: Short-Term vs. Long-Term

What we've implemented is **short-term memory**—conversation history that persists during a session. But agent memory systems can be much more sophisticated:

### Short-Term Memory (What We Built)

- Recent conversation turns
- Current session context
- Active task information
- Stored in-memory or cache
- Fast access, limited retention

### Long-Term Memory (Advanced)

- User preferences and patterns
- Historical interactions across sessions
- Learned facts and corrections
- Domain-specific knowledge
- Stored in databases or vector stores
- Persistent across sessions

### Working Memory (Task-Specific)

- Intermediate computation results
- Active tool outputs
- Temporary reasoning state
- Cleared after task completion

In production systems, you'd implement all three types, with strategies for deciding what to remember and what to forget.

## The Challenge: Context Window Limits

Our simple memory implementation has a hidden problem: **it grows without bounds.**

Every conversation turn adds to the messages list. After 100 turns, you're sending 200 messages (user + agent) on every request. Eventually, you'll hit the model's context window limit—the maximum amount of text it can process at once.

Common context window sizes:

- GPT-3.5: ~16K tokens
- GPT-4: ~8K-128K tokens (depending on version)
- Claude: ~100K-200K tokens

### Memory Management Strategies

**1. Sliding Window**
Keep only the last N conversation turns:

```python
MAX_TURNS = 10
messages = messages[-(MAX_TURNS * 2):]  # Keep last 10 user+agent pairs

```

**2. Summarization**
Periodically summarize old conversations and keep only the summary:

```python
if len(messages) > 20:
    summary = await summarize_conversation(messages[:10])
    messages = [{"role": "system", "content": summary}] + messages[10:]

```

**3. Semantic Filtering**
Keep only messages relevant to the current topic using embeddings and similarity search.

**4. Hierarchical Memory**

- Detailed recent memory (last few turns)
- Summarized medium-term memory
- Key facts in long-term storage

We'll explore these advanced techniques in later blogs when we build production-ready systems.

## Memory and Agent Identity

Memory doesn't just enable context—it shapes the agent's identity. With memory, your agent can:

**Build Relationships**

```
User: "My name is Sarah"
[Later in conversation]
Agent: "Sarah, based on what you told me earlier about your project..."

```

**Learn Preferences**

```
User: "Please keep responses concise"
[Subsequent responses become more concise]

```

**Maintain Consistency**

```
User: "What's our project timeline?"
Agent: [Refers back to earlier discussion of deadlines]

```

**Develop Understanding**

```
User: "Use the same approach we discussed"
Agent: [Recalls and applies the specific approach]

```

This transforms the agent from a tool into something closer to a colleague—an entity that knows you, understands your context, and adapts to your needs.

## Practical Considerations

### When to Clear Memory

Not all conversations should persist indefinitely:

- **Topic changes**: "Let's start over with a new topic"
- **Error recovery**: If the agent gets confused, reset context
- **Privacy**: Clear sensitive information after handling
- **Session boundaries**: New user sessions often need fresh context

You might implement a reset function:

```python
def reset_memory():
    global messages
    messages = []

```

### Memory Serialization

For production systems, memory needs to persist across sessions:

```python
import json

# Save memory
with open(f"memory_{user_id}.json", "w") as f:
    json.dump(messages, f)

# Load memory
with open(f"memory_{user_id}.json", "r") as f:
    messages = json.load(f)

```

### Multi-User Systems

In applications serving multiple users, you need isolated memory per user:

```python
user_memories = {}

async def ask_agent(user_id: str, query: str):
    if user_id not in user_memories:
        user_memories[user_id] = []

    messages = user_memories[user_id]
    input_msg = [{"role": "user", "content": query}]
    result = await Runner.run(custom_agent, input=messages + input_msg)
    user_memories[user_id] = result.to_input_list()
    return result

```

## The Foundation for Everything Else

Memory is the foundation that makes everything else in this blog series possible:

- **Tools**: Agents need memory to track tool results and multi-step operations
- **Handoffs**: Agents pass context to each other through memory
- **Orchestration**: Coordinating agents requires shared memory structures
- **Learning**: Agents improve by analyzing patterns in memory

Without memory, you have a fancy question-answering system. With memory, you have an agent.

## What We've Accomplished

Today, we transformed our agent from stateless responder to conversational partner:

- ✅ Implemented conversation memory storage
- ✅ Built a context-aware interaction function
- ✅ Demonstrated multi-turn conversations
- ✅ Explored memory structures and formats
- ✅ Understood context resolution mechanisms
- ✅ Learned about memory management challenges
- ✅ Laid the groundwork for advanced agent capabilities

## Try It Yourself

Experiment with your memory-enabled agent:

1. **Test pronoun resolution**: Ask about a topic, then use "it," "that," or "the thing we discussed"
2. **Build complex context**: Have a 10-turn conversation and see how well the agent maintains coherence
3. **Test memory limits**: Keep talking until you hit token limits—how does the agent behave?
4. **Try topic switching**: Change subjects abruptly—does the agent track the transition?
5. **Implement a reset command**: Add functionality to clear memory on command

## What's Next

We now have an agent that can remember and maintain context. But it's still limited to just talking—it can't act on the world.

In the next blog, we'll give our agent **tools**—the ability to search the web, run calculations, query databases, and interact with external systems. That's when our agent truly becomes powerful.

---

*Next up: Building Tools for AI Agents—teaching your agent to take action.*