# Understanding Multi-Modal AI

Multi-Modal AI refers to models that can process **multiple types of input data**—such as text, images, and audio—together. This unlocks new possibilities:

- Describing an image in natural language.
- Answering questions about a chart or infographic.
- Generating captions for a photo.
- Combining text context and visual cues for richer responses.

With Azure OpenAI’s GPT-4o and GPT-4o-mini models, you can send **text and images in the same request**. This enables tasks like:

- Explaining a diagram you upload.
- Extracting text from a screenshot.
- Recommending outfits based on a photo.

---

## **1. How Multi-Modal Inputs Work**

In the Chat Completions API, each `message` can contain multiple `content` parts.

- A **text part** is just a string.
- An **image part** can be sent by URL or as **base64-encoded data**.

Base64 encoding allows you to send an image directly in the API request without hosting it somewhere public.

---

## **2. Encoding an Image in Base64**

Python makes this simple:

```python
import base64

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

```

---

## **3. Sending Text + Image to Azure OpenAI**

Here’s how to modify your existing Azure OpenAI client setup to send an image and text together:

```python
import os
import base64
from dotenv import load_dotenv
from openai import AzureOpenAI

# Load environment variables
load_dotenv()

endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")

# Initialize client
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=api_key,
)

# Encode image to base64
image_path = "eiffel_tower.jpg"  # Replace with your image file
image_base64 = base64.b64encode(open(image_path, "rb").read()).decode("utf-8")

# Send a multi-modal request
response = client.chat.completions.create(
    model=deployment,
    max_tokens=1024,
    temperature=0.7,
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Describe the landmark in this picture."},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            ]
        }
    ]
)

# Print the model's description
print(response.choices[0].message.content)

```

---

### **Example Output**

```
The image shows the Eiffel Tower in Paris, France, standing tall against a clear blue sky.

```

---

## **4. Why Base64 Is Useful**

- **No external hosting required** — send images directly from local files.
- **Works in offline or restricted environments**.
- **Ensures privacy** — image never has to be uploaded to a public server before analysis.

---

## **5. Use Cases for Multi-Modal AI**

- **Travel & Education**: “Tell me about this historical landmark.”
- **E-commerce**: “Find similar products to this photo.”
- **Medical Imaging**: “Describe anomalies in this X-ray.” *(with proper compliance)*
- **Data Visualization**: “Explain this chart in simple terms.”

---

**Key Takeaway:**
Multi-Modal AI lets you combine **textual context** and **visual input** in a single request. With Azure OpenAI’s Chat Completions API, you can pass **base64-encoded images** alongside your text prompts to unlock richer, more capable applications.

---