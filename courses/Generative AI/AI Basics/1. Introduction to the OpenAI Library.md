# Introduction to the OpenAI Library

## 1. Setting Up Environment Variables with `.env`

Before hardcoding sensitive values like API keys and endpoints into your code, it’s best practice to store them in a `.env` file. This keeps your credentials secure and makes your code easier to maintain.

### 1.1 Create a `.env` File

In your project’s root directory, create a `.env` file with the following content:

```
AZURE_OPENAI_ENDPOINT=https://<your-endpoint>.openai.azure.com/
AZURE_OPENAI_API_KEY=<your-api-key>
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_DEPLOYMENT=<your-deployment-name>

```

**Notes:**

- Replace `<your-endpoint>` with the actual endpoint from your Azure OpenAI deployment.
- Replace `<your-api-key>` with your Azure API key.
- Replace `<your-deployment-name>` with the name of your deployed model (e.g., `gpt-4o`).

---

### 1.2 Loading Environment Variables in Python

We’ll use the [`python-dotenv`](https://pypi.org/project/python-dotenv/) package to load the `.env` file into our Python environment.

Install it:

```bash
pip install python-dotenv

```

Load variables in your code:

```python
import os
from dotenv import load_dotenv

# Load the .env file
load_dotenv()

# Access environment variables
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")

print("Environment variables loaded successfully!")

```

Now, you can reuse these variables across your application without exposing sensitive credentials directly in your source code.

---

# 2. Install Dependencies

Before diving into code, ensure that all the necessary dependencies are installed. The Azure OpenAI SDK requires **Python >=3.8**.

Install the SDK:

```bash
pip install openai

```

This installs the libraries needed to interact with the Azure OpenAI API.

---

# 3. Authentication and Client Setup

To authenticate and interact with the Azure OpenAI service, you’ll need:

- **API endpoint URL** – from your Azure OpenAI deployment.
- **API key** – tied to your Azure subscription.
- **API version** – e.g., `"2024-12-01-preview"`.
- **Deployment name** – the specific model you deployed.

Here’s how to initialize the Azure OpenAI client (loading credentials from `.env`):

```python
import os
from dotenv import load_dotenv
from openai import AzureOpenAI

# Load environment variables
load_dotenv()

endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")

# Initialize the client
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=api_key,
)

print("Azure OpenAI client initialized successfully!")

```

> You can find your endpoint and API key in the Azure Portal under the "Deployments + Endpoint" section after deploying your model.
> 

---

# 4. Run Your First Code Sample

### 4.1 Goal

We’ll send a **simple request** to the **Chat Completions API** using our deployment.

```python
import os
from dotenv import load_dotenv
from openai import AzureOpenAI

# Load environment variables
load_dotenv()

endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")

# Initialize client
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=api_key,
)

# Send a prompt to the model
response = client.chat.completions.create(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "I’m traveling to Paris. What should I see?"},
    ],
    max_tokens=4096,
    temperature=1.0,
    top_p=1.0,
    model=deployment,
)

# Print the response
print(response.choices[0].message.content)

```

---

# 5. Explore More Samples

## 5.1 Multi-Turn Conversation

For chat apps, you’ll often want to **maintain conversation history**:

```python
response = client.chat.completions.create(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What’s worth seeing in Paris?"},
        {"role": "assistant", "content": "You should visit attractions such as the Eiffel Tower, the Louvre, and Notre-Dame Cathedral."},
        {"role": "user", "content": "What’s special about the Eiffel Tower?"},
    ],
    max_tokens=4096,
    temperature=1.0,
    top_p=1.0,
    model=deployment,
)

print(response.choices[0].message.content)

```

---

## 5.2 Streaming Responses in Real-Time

To create a **real-time streaming effect**, print tokens as they’re generated:

```python
response = client.chat.completions.create(
    stream=True,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What should I visit in Paris?"},
    ],
    max_tokens=4096,
    temperature=1.0,
    top_p=1.0,
    model=deployment,
)

for update in response:
    if update.choices:
        print(update.choices[0].delta.content or "", end="")

client.close()

```

---